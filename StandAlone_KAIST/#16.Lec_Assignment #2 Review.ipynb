{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"#16.Lec_Assignment #2 Review.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python (comet2)","language":"python","name":"comet2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"code","metadata":{"id":"PD4cIKKvKFCC"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import argparse\n","import numpy as np\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29UainWPco7Y"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"id":"Cu753dPPKGkV"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","partition = {'train': trainset, 'val':valset, 'test':testset}               # dataset을 dictionary로 만듦"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxnfFJwBcsAv"},"source":["## Model Architecture  ★★★"]},{"cell_type":"code","metadata":{"id":"_G6bZbbkMWWt"},"source":["class MLP(nn.Module):\n","    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):    # added dropout, use_bn, use_xavier\n","        super(MLP, self).__init__()\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.hid_dim = hid_dim\n","        self.n_layer = n_layer\n","        self.act = act\n","        self.dropout = dropout              # 0.0 ~ 1.0 float value\n","        self.use_bn = use_bn                # True/False\n","        self.use_xavier = use_xavier        # True/False\n","        \n","        # ====== Create Linear Layers ====== #\n","        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n","        \n","        self.linears = nn.ModuleList()\n","        self.bns = nn.ModuleList()\n","        for i in range(self.n_layer-1):\n","            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n","            if self.use_bn:                                                 # Batch Norm 안에 Trainable 파라미터가 존재하기 때문에 하나의 Batch Norm later는 하나의 linear layer를 담당해야함\n","                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n","                \n","        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n","        \n","        # ====== Create Activation Function ====== #\n","        if self.act == 'relu':\n","            self.act = nn.ReLU()\n","        elif self.act == 'tanh':\n","            self.act == nn.Tanh()\n","        elif self.act == 'sigmoid':\n","            self.act = nn.Sigmoid()\n","        else:\n","            raise ValueError('no valid activation function selected!')          \n","        \n","        # ====== Create Regularization Layer ======= #\n","        self.dropout = nn.Dropout(self.dropout)\n","        if self.use_xavier:\n","            self.xavier_init()\n","          \n","    '''\n","    BatchNomalization이 먼저일지 Dropout이 먼저일지 생각해보면,\n","    BN의 역할이 모델이 만들어낸 x들의 평균과 분산을 학습해서 Nomalize해주는 역할인데,\n","    만약 Dropout이 먼저 와버리면 랜덤으로 노드들을 끄다 보니깐 원래 평균/분산이 정확하게 Nomarlize되지 않음.\n","    결론: BN된 x들중에 DropOut을 해줘야 정상적인 학습을 할 수 있다.\n","    '''\n","    def forward(self, x):\n","        x = self.act(self.fc1(x))\n","        for i in range(len(self.linears)):\n","            x = self.act(self.linears[i](x))\n","            x = self.bns[i](x)                      # bn은 모델이 만들어낸 x의 평균과 분산을 학습해서 nomalization을 해주기 때문에 dropout 전에 해줘야 정상 작동이 됨 \n","            x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","    \n","    def xavier_init(self):\n","        for linear in self.linears:\n","            nn.init.xavier_normal_(linear.weight)   # Initialize w\n","            linear.bias.data.fill_(0.01)            # Initialize b to 0.01\n","            \n","net = MLP(3072, 10, 100, 4, 'relu', 0.1, True, True) # Testing Model Construction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itGsp6jDWs_a"},"source":["## Train, Validate, Test and Experiment"]},{"cell_type":"code","metadata":{"id":"Oe5bXlAjeQGh"},"source":["def train(net, partition, optimizer, criterion, args):\n","    trainloader = torch.utils.data.DataLoader(partition['train'], \n","                                              batch_size=args.train_batch_size, \n","                                              shuffle=True, num_workers=2)\n","    net.train()\n","    optimizer.zero_grad()\n","\n","    correct = 0\n","    total = 0\n","    train_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs = inputs.view(-1, 3072)\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        outputs = net(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    train_loss = train_loss / len(trainloader)\n","    train_acc = 100 * correct / total\n","    return net, train_loss, train_acc       # net: Train하면서 학습된 net, loss, acc: Overfitting 유무를 위해 둘다 return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeGMCb_0eQGi"},"source":["def validate(net, partition, criterion, args):      # parameter 업데이트는 없으므로 optimizer는 필요 없음\n","    valloader = torch.utils.data.DataLoader(partition['val'], \n","                                            batch_size=args.test_batch_size, \n","                                            shuffle=False, num_workers=2)\n","    net.eval()\n","\n","    correct = 0\n","    total = 0\n","    val_loss = 0 \n","    with torch.no_grad():\n","        for data in valloader:\n","            images, labels = data\n","            images = images.view(-1, 3072)\n","            images = images.cuda()\n","            labels = labels.cuda()\n","            outputs = net(images)\n","\n","            loss = criterion(outputs, labels)\n","            \n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(valloader)\n","        val_acc = 100 * correct / total\n","    return val_loss, val_acc        # 학습은 이루어 지지 않으므로 net return은 할 필요 없음"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcSsG64LeQGi"},"source":["def test(net, partition, args):\n","    testloader = torch.utils.data.DataLoader(partition['test'], \n","                                             batch_size=args.test_batch_size,       # batch size도 hyperparameter로 다루기 위함\n","                                             shuffle=False,\n","                                             num_workers=2)\n","    net.eval()\n","    \n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            images = images.view(-1, 3072)\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","            outputs = net(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        test_acc = 100 * correct / total\n","    return test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiOCP6TqWw2V"},"source":["def experiment(partition, args):\n","  \n","    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.use_xavier)\n","    net.cuda()\n","\n","    criterion = nn.CrossEntropyLoss()\n","    if args.optim == 'SGD':\n","        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'RMSprop':\n","        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    elif args.optim == 'Adam':\n","        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n","    else:\n","        raise ValueError('In-valid optimizer choice')\n","    \n","    for epoch in range(args.epoch):     # loop over the dataset multiple times\n","        ts = time.time()                # 시간 측정\n","        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n","        val_loss, val_acc = validate(net, partition, criterion, args)\n","        te = time.time()\n","        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n","        \n","    test_acc = test(net, partition, args)    \n","    return train_loss, val_loss, train_acc, val_acc, test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omgExzmQgU1J"},"source":["## Experiment"]},{"cell_type":"code","metadata":{"id":"DRoOy_B3Wu7B"},"source":["# ====== Random Seed Initialization ====== #\n","seed = 123\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","parser = argparse.ArgumentParser()\n","args = parser.parse_args(\"\")\n","\n","# ====== Model Capacity ====== #\n","args.in_dim = 3072\n","args.out_dim = 10\n","args.hid_dim = 100\n","args.act = 'relu'\n","\n","# ====== Regularization ======= #\n","args.dropout = 0.2\n","args.use_bn = True\n","args.l2 = 0.00001\n","args.use_xavier = True\n","\n","# ====== Optimizer & Training ====== #\n","args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n","args.lr = 0.0015\n","args.epoch = 10\n","\n","args.train_batch_size = 256\n","args.test_batch_size = 1024\n","\n","# ====== Experiment Variable ====== #\n","name_var1 = 'n_layer'\n","name_var2 = 'hid_dim'\n","list_var1 = [3, 3, 4]\n","list_var2 = [500, 300, 700]\n","\n","\n","for var1 in list_var1:\n","    for var2 in list_var2:\n","        setattr(args, name_var1, var1)\n","        setattr(args, name_var2, var2)\n","        print(args)\n","        result = experiment(partition, args)  "],"execution_count":null,"outputs":[]}]}